{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:51<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7127260398691052\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:59<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0767094462687161\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:42<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.028270753643309776\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:24<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.01557112114641107\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:24<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.10099800203018393\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:27<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.04084696591706044\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:39<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.00813510620600881\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [11:48<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.00668761181713659\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 97/163 [07:08<05:21,  4.87s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取文件\n",
    "file_path = '1.csv'  # 请将路径替换为您的文件路径\n",
    "data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 确定标签数量\n",
    "num_labels = len(data['label'].unique())\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess(data, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        data['question'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = torch.tensor(data['label'].tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 预处理训练和验证数据\n",
    "train_encodings, train_labels = preprocess(train_data, tokenizer)\n",
    "val_encodings, val_labels = preprocess(val_data, tokenizer)\n",
    "\n",
    "# 创建Dataset对象\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# 加载BERT模型\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "\n",
    "# 选择优化器和损失函数\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Average training loss: {avg_loss}')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Loss: {avg_val_loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# 保存模型权重\n",
    "model_save_path = 'bert_model_weights.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model weights saved to {model_save_path}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-06T08:18:33.477754Z"
    }
   },
   "id": "16bb91a89dd7134b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be3578cb6600a31a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to bert_model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'bert_model_weights.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model weights saved to {model_save_path}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T08:13:46.943365Z",
     "start_time": "2024-06-06T08:13:46.505367Z"
    }
   },
   "id": "51e1036c1a69e531",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# 读取文件\n",
    "file_path = '1.csv'  # 请将路径替换为您的文件路径\n",
    "data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 确定标签数量\n",
    "num_labels = len(data['label'].unique())\n",
    "# 加载tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_labels)  # 确保num_labels定义正确\n",
    "\n",
    "# 加载保存的模型权重\n",
    "model_path = 'bert_model_weights.pth'  # 请将路径替换为保存模型权重的路径\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    print(f'Model weights loaded from {model_path}')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "# 定义预测函数\n",
    "def predict(sentence):\n",
    "    # 预处理输入句子\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取预测结果\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class_id\n",
    "\n",
    "# 测试预测函数\n",
    "sentence = \"请问你知道李世民的封号吗？\"\n",
    "predicted_class = predict(sentence)\n",
    "print(f\"预测类别: {predicted_class}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "71723a1c0b9b6df0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [04:23<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.22188058434163824\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [04:23<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.04733705812079065\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [04:22<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.026171437979621046\n",
      "Validation Loss: 0.020316480912945488\n",
      "Validation Accuracy: 0.9985294117647059\n",
      "Model weights saved to bert_model_weights_multi.pth\n",
      "Test Predictions:\n",
      "Sentence: 你知道李鸿章的官职和出生日期吗？\n",
      "Predicted Labels: ['1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 读取文件\n",
    "file_path = 'sentences.csv'  # 请将路径替换为您的文件路径\n",
    "data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 假设标签是以逗号分隔的字符串，如 'label1,label2'\n",
    "# 将标签转换为多热编码\n",
    "all_labels = set()\n",
    "\n",
    "def process_labels(label):\n",
    "    if isinstance(label, int):\n",
    "        label = str(label)\n",
    "    return label.split(',')\n",
    "\n",
    "data['label'] = data['label'].apply(process_labels)\n",
    "for labels in data['label']:\n",
    "    all_labels.update(labels)\n",
    "all_labels = list(all_labels)\n",
    "label_map = {label: i for i, label in enumerate(all_labels)}\n",
    "\n",
    "data['label'] = data['label'].apply(lambda x: [label_map[label] for label in x])\n",
    "data['label'] = data['label'].apply(lambda x: np.eye(len(all_labels))[x].sum(axis=0))\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess(data, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        data['question'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = torch.tensor(data['label'].tolist(), dtype=torch.float32)\n",
    "    return encodings, labels\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 预处理训练和验证数据\n",
    "train_encodings, train_labels = preprocess(train_data, tokenizer)\n",
    "val_encodings, val_labels = preprocess(val_data, tokenizer)\n",
    "\n",
    "# 创建Dataset对象\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# 加载BERT模型并添加一个全连接层\n",
    "class BertForMultiLabelClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BertForMultiLabelClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return logits if loss is None else (loss, logits)\n",
    "\n",
    "model = BertForMultiLabelClassification('bert-base-chinese', num_labels=len(all_labels))\n",
    "\n",
    "# 选择优化器\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        loss, outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Average training loss: {avg_loss}')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "total = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        loss, outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "accuracy = correct_predictions / total\n",
    "\n",
    "print(f'Validation Loss: {avg_val_loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# 保存模型权重\n",
    "model_save_path = 'bert_model_weights_multi.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model weights saved to {model_save_path}')\n",
    "\n",
    "# 输出多分类数组\n",
    "test_data = [\"你知道李鸿章的官职和出生日期吗？\"]  # 替换为您的测试数据\n",
    "test_encodings = tokenizer(test_data, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_mask = test_encodings['attention_mask']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_input_ids, test_attention_mask)\n",
    "    test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "    print(\"Test Predictions:\")\n",
    "    for i, prediction in enumerate(test_predictions):\n",
    "        labels = [all_labels[j] for j, val in enumerate(prediction) if val]\n",
    "        print(f\"Sentence: {test_data[i]}\")\n",
    "        print(f\"Predicted Labels: {labels}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T02:09:59.996405Z",
     "start_time": "2024-06-06T01:56:26.721456Z"
    }
   },
   "id": "c10e91c42a6e54be",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([15, 768]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([16]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbert_model_weights_multi.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m test_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m你知道李鸿章的官职和出生日期吗？\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# 替换为您的测试数据\u001B[39;00m\n\u001B[0;32m      3\u001B[0m test_encodings \u001B[38;5;241m=\u001B[39m tokenizer(test_data, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m'\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\history\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2184\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2185\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2186\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   2188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2189\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2190\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   2191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([15, 768]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([16])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bert_model_weights_multi.pth'))\n",
    "test_data = [\"你知道李鸿章的官职和出生日期吗？\"]  # 替换为您的测试数据\n",
    "test_encodings = tokenizer(test_data, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_mask = test_encodings['attention_mask']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_input_ids, test_attention_mask)\n",
    "    test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "    print(\"Test Predictions:\")\n",
    "    for i, prediction in enumerate(test_predictions):\n",
    "        labels = [all_labels[j] for j, val in enumerate(prediction) if val]\n",
    "        print(f\"Sentence: {test_data[i]}\")\n",
    "        print(f\"Predicted Labels: {labels}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T08:13:47.543361Z",
     "start_time": "2024-06-06T08:13:46.944364Z"
    }
   },
   "id": "a7471c06ad3ec63a",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     question  label\n",
      "0  请问a的籍贯是哪里？      0\n",
      "1   你知道a的籍贯吗？      0\n",
      "2      a是哪的人？      0\n",
      "3      a是哪里人？      0\n",
      "4    a的家乡在哪里？      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\history\\venv\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 15]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 78\u001B[0m\n\u001B[0;32m     75\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     77\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask)\n\u001B[1;32m---> 78\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBCEWithLogitsLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     80\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mE:\\history\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\history\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\history\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:731\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 731\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy_with_logits\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    732\u001B[0m \u001B[43m                                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    733\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\history\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3224\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   3221\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[1;32m-> 3224\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3226\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\u001B[38;5;28minput\u001B[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 15]))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# 读取上传的CSV文件\n",
    "file_path = '1.csv'\n",
    "data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 查看数据\n",
    "print(data.head())\n",
    "\n",
    "# 假设我们有两个标签：籍贯(0)和出生年份(1)\n",
    "num_labels = len(data['label'].unique())\n",
    "\n",
    "# 创建多标签向量\n",
    "def create_multi_label(row):\n",
    "    multi_label = [0] * num_labels\n",
    "    multi_label[row['label']] = 1\n",
    "    return multi_label\n",
    "\n",
    "data['multi_label'] = data.apply(create_multi_label, axis=1)\n",
    "\n",
    "# 准备数据集\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = labels\n",
    "\n",
    "        return item\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 创建数据集\n",
    "texts = data['question'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "max_len = 128\n",
    "\n",
    "dataset = MultiLabelDataset(texts, labels, tokenizer, max_len)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = 'bert_multi_label_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model weights saved to {model_save_path}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T06:06:28.435707Z",
     "start_time": "2024-06-06T06:06:25.901874Z"
    }
   },
   "id": "503c6389d84bb1d5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     question  label\n",
      "0  请问a的籍贯是哪里？      0\n",
      "1   你知道a的籍贯吗？      0\n",
      "2      a是哪的人？      0\n",
      "3      a是哪里人？      0\n",
      "4    a的家乡在哪里？      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\history\\venv\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7667551040649414\n",
      "Epoch 0, Loss: 0.6823835372924805\n",
      "Epoch 0, Loss: 0.616031289100647\n",
      "Epoch 0, Loss: 0.5834007859230042\n",
      "Epoch 0, Loss: 0.5292381644248962\n",
      "Epoch 0, Loss: 0.4958725869655609\n",
      "Epoch 0, Loss: 0.4733896255493164\n",
      "Epoch 0, Loss: 0.4600932002067566\n",
      "Epoch 0, Loss: 0.4359320104122162\n",
      "Epoch 0, Loss: 0.4236968159675598\n",
      "Epoch 0, Loss: 0.4078444242477417\n",
      "Epoch 0, Loss: 0.38233062624931335\n",
      "Epoch 0, Loss: 0.3815326690673828\n",
      "Epoch 0, Loss: 0.37401801347732544\n",
      "Epoch 0, Loss: 0.37017694115638733\n",
      "Epoch 0, Loss: 0.35244953632354736\n",
      "Epoch 0, Loss: 0.34605684876441956\n",
      "Epoch 0, Loss: 0.3288227319717407\n",
      "Epoch 0, Loss: 0.3219880759716034\n",
      "Epoch 0, Loss: 0.3169013559818268\n",
      "Epoch 0, Loss: 0.30686473846435547\n",
      "Epoch 0, Loss: 0.30512863397598267\n",
      "Epoch 0, Loss: 0.29622724652290344\n",
      "Epoch 0, Loss: 0.279869019985199\n",
      "Epoch 0, Loss: 0.301534503698349\n",
      "Epoch 0, Loss: 0.28427669405937195\n",
      "Epoch 0, Loss: 0.27181658148765564\n",
      "Epoch 0, Loss: 0.273240864276886\n",
      "Epoch 0, Loss: 0.2680374085903168\n",
      "Epoch 0, Loss: 0.2795369029045105\n",
      "Epoch 0, Loss: 0.2759256958961487\n",
      "Epoch 0, Loss: 0.2553580105304718\n",
      "Epoch 0, Loss: 0.2653772532939911\n",
      "Epoch 0, Loss: 0.2633511424064636\n",
      "Epoch 0, Loss: 0.25267696380615234\n",
      "Epoch 0, Loss: 0.26858991384506226\n",
      "Epoch 0, Loss: 0.24499957263469696\n",
      "Epoch 0, Loss: 0.26010483503341675\n",
      "Epoch 0, Loss: 0.2433566153049469\n",
      "Epoch 0, Loss: 0.23599080741405487\n",
      "Epoch 0, Loss: 0.2493409961462021\n",
      "Epoch 0, Loss: 0.23616819083690643\n",
      "Epoch 0, Loss: 0.24436169862747192\n",
      "Epoch 0, Loss: 0.22581638395786285\n",
      "Epoch 0, Loss: 0.23699726164340973\n",
      "Epoch 0, Loss: 0.23392809927463531\n",
      "Epoch 0, Loss: 0.24406853318214417\n",
      "Epoch 0, Loss: 0.23276640474796295\n",
      "Epoch 0, Loss: 0.24009376764297485\n",
      "Epoch 0, Loss: 0.23442453145980835\n",
      "Epoch 0, Loss: 0.22177426517009735\n",
      "Epoch 0, Loss: 0.23194588720798492\n",
      "Epoch 0, Loss: 0.23925155401229858\n",
      "Epoch 0, Loss: 0.23154740035533905\n",
      "Epoch 0, Loss: 0.2263774275779724\n",
      "Epoch 0, Loss: 0.21528421342372894\n",
      "Epoch 0, Loss: 0.20913365483283997\n",
      "Epoch 0, Loss: 0.20678067207336426\n",
      "Epoch 0, Loss: 0.21250057220458984\n",
      "Epoch 0, Loss: 0.21151360869407654\n",
      "Epoch 0, Loss: 0.19819951057434082\n",
      "Epoch 0, Loss: 0.20870555937290192\n",
      "Epoch 0, Loss: 0.20733459293842316\n",
      "Epoch 0, Loss: 0.20112985372543335\n",
      "Epoch 0, Loss: 0.21200081706047058\n",
      "Epoch 0, Loss: 0.204761803150177\n",
      "Epoch 0, Loss: 0.20003581047058105\n",
      "Epoch 0, Loss: 0.20461758971214294\n",
      "Epoch 0, Loss: 0.18214108049869537\n",
      "Epoch 0, Loss: 0.19506096839904785\n",
      "Epoch 0, Loss: 0.19225956499576569\n",
      "Epoch 0, Loss: 0.18229523301124573\n",
      "Epoch 0, Loss: 0.19752074778079987\n",
      "Epoch 0, Loss: 0.17863477766513824\n",
      "Epoch 0, Loss: 0.17864656448364258\n",
      "Epoch 0, Loss: 0.1979496330022812\n",
      "Epoch 0, Loss: 0.18630769848823547\n",
      "Epoch 0, Loss: 0.15880489349365234\n",
      "Epoch 0, Loss: 0.1832665652036667\n",
      "Epoch 0, Loss: 0.18959535658359528\n",
      "Epoch 0, Loss: 0.17550016939640045\n",
      "Epoch 0, Loss: 0.16213884949684143\n",
      "Epoch 0, Loss: 0.1616196632385254\n",
      "Epoch 0, Loss: 0.15779231488704681\n",
      "Epoch 0, Loss: 0.1794106364250183\n",
      "Epoch 0, Loss: 0.15801987051963806\n",
      "Epoch 0, Loss: 0.15294048190116882\n",
      "Epoch 0, Loss: 0.15133969485759735\n",
      "Epoch 0, Loss: 0.1402466744184494\n",
      "Epoch 0, Loss: 0.14336267113685608\n",
      "Epoch 0, Loss: 0.15186408162117004\n",
      "Epoch 0, Loss: 0.15459607541561127\n",
      "Epoch 0, Loss: 0.14182724058628082\n",
      "Epoch 0, Loss: 0.13187362253665924\n",
      "Epoch 0, Loss: 0.15402553975582123\n",
      "Epoch 0, Loss: 0.1483188271522522\n",
      "Epoch 0, Loss: 0.1393842101097107\n",
      "Epoch 0, Loss: 0.15055543184280396\n",
      "Epoch 0, Loss: 0.14194928109645844\n",
      "Epoch 0, Loss: 0.14865116775035858\n",
      "Epoch 0, Loss: 0.1410049945116043\n",
      "Epoch 0, Loss: 0.13621050119400024\n",
      "Epoch 0, Loss: 0.12684781849384308\n",
      "Epoch 0, Loss: 0.13382580876350403\n",
      "Epoch 0, Loss: 0.13704326748847961\n",
      "Epoch 0, Loss: 0.12863251566886902\n",
      "Epoch 0, Loss: 0.13788138329982758\n",
      "Epoch 0, Loss: 0.12333425879478455\n",
      "Epoch 0, Loss: 0.12322894483804703\n",
      "Epoch 0, Loss: 0.14089274406433105\n",
      "Epoch 0, Loss: 0.12780435383319855\n",
      "Epoch 0, Loss: 0.12468443810939789\n",
      "Epoch 0, Loss: 0.13492806255817413\n",
      "Epoch 0, Loss: 0.11994660645723343\n",
      "Epoch 0, Loss: 0.11680781841278076\n",
      "Epoch 0, Loss: 0.10962793231010437\n",
      "Epoch 0, Loss: 0.11806783825159073\n",
      "Epoch 0, Loss: 0.11145363003015518\n",
      "Epoch 0, Loss: 0.11703624576330185\n",
      "Epoch 0, Loss: 0.13242802023887634\n",
      "Epoch 0, Loss: 0.12636798620224\n",
      "Epoch 0, Loss: 0.10757220536470413\n",
      "Epoch 0, Loss: 0.12039493769407272\n",
      "Epoch 0, Loss: 0.12262937426567078\n",
      "Epoch 0, Loss: 0.120963454246521\n",
      "Epoch 0, Loss: 0.12019266933202744\n",
      "Epoch 0, Loss: 0.11016030609607697\n",
      "Epoch 0, Loss: 0.10939677804708481\n",
      "Epoch 0, Loss: 0.09823667258024216\n",
      "Epoch 0, Loss: 0.10788428038358688\n",
      "Epoch 0, Loss: 0.096516914665699\n",
      "Epoch 0, Loss: 0.11489405483007431\n",
      "Epoch 0, Loss: 0.11063823848962784\n",
      "Epoch 0, Loss: 0.11714141815900803\n",
      "Epoch 0, Loss: 0.0959167331457138\n",
      "Epoch 0, Loss: 0.08597170561552048\n",
      "Epoch 0, Loss: 0.0888417586684227\n",
      "Epoch 0, Loss: 0.10347437858581543\n",
      "Epoch 0, Loss: 0.10672283917665482\n",
      "Epoch 0, Loss: 0.09887346625328064\n",
      "Epoch 0, Loss: 0.09414221346378326\n",
      "Epoch 0, Loss: 0.08432614058256149\n",
      "Epoch 0, Loss: 0.09975870698690414\n",
      "Epoch 0, Loss: 0.09534440189599991\n",
      "Epoch 0, Loss: 0.10128722339868546\n",
      "Epoch 0, Loss: 0.09995115548372269\n",
      "Epoch 0, Loss: 0.08353854715824127\n",
      "Epoch 0, Loss: 0.07972577959299088\n",
      "Epoch 0, Loss: 0.10245494544506073\n",
      "Epoch 0, Loss: 0.08915115147829056\n",
      "Epoch 0, Loss: 0.08839143067598343\n",
      "Epoch 0, Loss: 0.1197492778301239\n",
      "Epoch 0, Loss: 0.08278949558734894\n",
      "Epoch 0, Loss: 0.09098310768604279\n",
      "Epoch 0, Loss: 0.09120044112205505\n",
      "Epoch 0, Loss: 0.08784417808055878\n",
      "Epoch 0, Loss: 0.10117907077074051\n",
      "Epoch 0, Loss: 0.09620677679777145\n",
      "Epoch 0, Loss: 0.08955809473991394\n",
      "Epoch 0, Loss: 0.08086859434843063\n",
      "Epoch 0, Loss: 0.09241309016942978\n",
      "Epoch 0, Loss: 0.07603146135807037\n",
      "Epoch 0, Loss: 0.07396776229143143\n",
      "Epoch 0, Loss: 0.08717511594295502\n",
      "Epoch 0, Loss: 0.07626794278621674\n",
      "Epoch 0, Loss: 0.08132950216531754\n",
      "Epoch 0, Loss: 0.10264299809932709\n",
      "Epoch 0, Loss: 0.07764315605163574\n",
      "Epoch 0, Loss: 0.08074810355901718\n",
      "Epoch 0, Loss: 0.08119950443506241\n",
      "Epoch 0, Loss: 0.08184932172298431\n",
      "Epoch 0, Loss: 0.06955046951770782\n",
      "Epoch 0, Loss: 0.07840254157781601\n",
      "Epoch 0, Loss: 0.09120626747608185\n",
      "Epoch 0, Loss: 0.08443878591060638\n",
      "Epoch 0, Loss: 0.09087453782558441\n",
      "Epoch 0, Loss: 0.07795752584934235\n",
      "Epoch 0, Loss: 0.08512969315052032\n",
      "Epoch 0, Loss: 0.07667476683855057\n",
      "Epoch 0, Loss: 0.07259581983089447\n",
      "Epoch 0, Loss: 0.06785761564970016\n",
      "Epoch 0, Loss: 0.08238089829683304\n",
      "Epoch 0, Loss: 0.0943937748670578\n",
      "Epoch 0, Loss: 0.06507421284914017\n",
      "Epoch 0, Loss: 0.07418713718652725\n",
      "Epoch 0, Loss: 0.07316894829273224\n",
      "Epoch 0, Loss: 0.07080401480197906\n",
      "Epoch 0, Loss: 0.06801909953355789\n",
      "Epoch 0, Loss: 0.08001268655061722\n",
      "Epoch 0, Loss: 0.07732362300157547\n",
      "Epoch 0, Loss: 0.07763376086950302\n",
      "Epoch 1, Loss: 0.059286847710609436\n",
      "Epoch 1, Loss: 0.06895390897989273\n",
      "Epoch 1, Loss: 0.07213396579027176\n",
      "Epoch 1, Loss: 0.057232391089200974\n",
      "Epoch 1, Loss: 0.0944519191980362\n",
      "Epoch 1, Loss: 0.06486370414495468\n",
      "Epoch 1, Loss: 0.07294242829084396\n",
      "Epoch 1, Loss: 0.061072271317243576\n",
      "Epoch 1, Loss: 0.06995042413473129\n",
      "Epoch 1, Loss: 0.06237716227769852\n",
      "Epoch 1, Loss: 0.06345818936824799\n",
      "Epoch 1, Loss: 0.05756749212741852\n",
      "Epoch 1, Loss: 0.060950469225645065\n",
      "Epoch 1, Loss: 0.05692293867468834\n",
      "Epoch 1, Loss: 0.05666779726743698\n",
      "Epoch 1, Loss: 0.06510046124458313\n",
      "Epoch 1, Loss: 0.06328346580266953\n",
      "Epoch 1, Loss: 0.06365305185317993\n",
      "Epoch 1, Loss: 0.060919713228940964\n",
      "Epoch 1, Loss: 0.07308542728424072\n",
      "Epoch 1, Loss: 0.05848307907581329\n",
      "Epoch 1, Loss: 0.05683039128780365\n",
      "Epoch 1, Loss: 0.06057854741811752\n",
      "Epoch 1, Loss: 0.058096934109926224\n",
      "Epoch 1, Loss: 0.06411801278591156\n",
      "Epoch 1, Loss: 0.05768728628754616\n",
      "Epoch 1, Loss: 0.06047302111983299\n",
      "Epoch 1, Loss: 0.06211654096841812\n",
      "Epoch 1, Loss: 0.05294622853398323\n",
      "Epoch 1, Loss: 0.0553189180791378\n",
      "Epoch 1, Loss: 0.05803787708282471\n",
      "Epoch 1, Loss: 0.054677121341228485\n",
      "Epoch 1, Loss: 0.05716637894511223\n",
      "Epoch 1, Loss: 0.051880378276109695\n",
      "Epoch 1, Loss: 0.06283745914697647\n",
      "Epoch 1, Loss: 0.05677957832813263\n",
      "Epoch 1, Loss: 0.05665631219744682\n",
      "Epoch 1, Loss: 0.058617137372493744\n",
      "Epoch 1, Loss: 0.05468261241912842\n",
      "Epoch 1, Loss: 0.056163039058446884\n",
      "Epoch 1, Loss: 0.05327136814594269\n",
      "Epoch 1, Loss: 0.051213085651397705\n",
      "Epoch 1, Loss: 0.052370600402355194\n",
      "Epoch 1, Loss: 0.061126384884119034\n",
      "Epoch 1, Loss: 0.04809493198990822\n",
      "Epoch 1, Loss: 0.04900171235203743\n",
      "Epoch 1, Loss: 0.051221270114183426\n",
      "Epoch 1, Loss: 0.049234528094530106\n",
      "Epoch 1, Loss: 0.053565703332424164\n",
      "Epoch 1, Loss: 0.051000770181417465\n",
      "Epoch 1, Loss: 0.04440328851342201\n",
      "Epoch 1, Loss: 0.049122169613838196\n",
      "Epoch 1, Loss: 0.048164233565330505\n",
      "Epoch 1, Loss: 0.049720343202352524\n",
      "Epoch 1, Loss: 0.04961703345179558\n",
      "Epoch 1, Loss: 0.046674832701683044\n",
      "Epoch 1, Loss: 0.047089044004678726\n",
      "Epoch 1, Loss: 0.048450469970703125\n",
      "Epoch 1, Loss: 0.046008650213479996\n",
      "Epoch 1, Loss: 0.048537056893110275\n",
      "Epoch 1, Loss: 0.04576478153467178\n",
      "Epoch 1, Loss: 0.048333726823329926\n",
      "Epoch 1, Loss: 0.045011408627033234\n",
      "Epoch 1, Loss: 0.045549023896455765\n",
      "Epoch 1, Loss: 0.043671540915966034\n",
      "Epoch 1, Loss: 0.04532104730606079\n",
      "Epoch 1, Loss: 0.04141588881611824\n",
      "Epoch 1, Loss: 0.042763907462358475\n",
      "Epoch 1, Loss: 0.0413721427321434\n",
      "Epoch 1, Loss: 0.043770160526037216\n",
      "Epoch 1, Loss: 0.042926691472530365\n",
      "Epoch 1, Loss: 0.04514351114630699\n",
      "Epoch 1, Loss: 0.04577551782131195\n",
      "Epoch 1, Loss: 0.04417625442147255\n",
      "Epoch 1, Loss: 0.04730250686407089\n",
      "Epoch 1, Loss: 0.039889510720968246\n",
      "Epoch 1, Loss: 0.04361962154507637\n",
      "Epoch 1, Loss: 0.041416190564632416\n",
      "Epoch 1, Loss: 0.04189001023769379\n",
      "Epoch 1, Loss: 0.043569400906562805\n",
      "Epoch 1, Loss: 0.0399991050362587\n",
      "Epoch 1, Loss: 0.07404231280088425\n",
      "Epoch 1, Loss: 0.039806775748729706\n",
      "Epoch 1, Loss: 0.04010187089443207\n",
      "Epoch 1, Loss: 0.042074207216501236\n",
      "Epoch 1, Loss: 0.0440271757543087\n",
      "Epoch 1, Loss: 0.038516879081726074\n",
      "Epoch 1, Loss: 0.04279793053865433\n",
      "Epoch 1, Loss: 0.040876127779483795\n",
      "Epoch 1, Loss: 0.04589002951979637\n",
      "Epoch 1, Loss: 0.03993641212582588\n",
      "Epoch 1, Loss: 0.038933735340833664\n",
      "Epoch 1, Loss: 0.06578375399112701\n",
      "Epoch 1, Loss: 0.04382846876978874\n",
      "Epoch 1, Loss: 0.041747208684682846\n",
      "Epoch 1, Loss: 0.038626592606306076\n",
      "Epoch 1, Loss: 0.039443239569664\n",
      "Epoch 1, Loss: 0.03806855157017708\n",
      "Epoch 1, Loss: 0.038137730211019516\n",
      "Epoch 1, Loss: 0.04177908971905708\n",
      "Epoch 1, Loss: 0.043480828404426575\n",
      "Epoch 1, Loss: 0.03997593745589256\n",
      "Epoch 1, Loss: 0.038156818598508835\n",
      "Epoch 1, Loss: 0.04264460504055023\n",
      "Epoch 1, Loss: 0.04044979065656662\n",
      "Epoch 1, Loss: 0.035620346665382385\n",
      "Epoch 1, Loss: 0.036999356001615524\n",
      "Epoch 1, Loss: 0.03724296763539314\n",
      "Epoch 1, Loss: 0.04020891338586807\n",
      "Epoch 1, Loss: 0.04036039113998413\n",
      "Epoch 1, Loss: 0.0375395268201828\n",
      "Epoch 1, Loss: 0.03684332221746445\n",
      "Epoch 1, Loss: 0.03669312223792076\n",
      "Epoch 1, Loss: 0.03881088271737099\n",
      "Epoch 1, Loss: 0.04481028765439987\n",
      "Epoch 1, Loss: 0.03466342017054558\n",
      "Epoch 1, Loss: 0.0397975780069828\n",
      "Epoch 1, Loss: 0.03731463849544525\n",
      "Epoch 1, Loss: 0.03784947469830513\n",
      "Epoch 1, Loss: 0.035456910729408264\n",
      "Epoch 1, Loss: 0.03770609572529793\n",
      "Epoch 1, Loss: 0.037238992750644684\n",
      "Epoch 1, Loss: 0.03563595563173294\n",
      "Epoch 1, Loss: 0.06341185420751572\n",
      "Epoch 1, Loss: 0.03775997459888458\n",
      "Epoch 1, Loss: 0.03855453059077263\n",
      "Epoch 1, Loss: 0.035754647105932236\n",
      "Epoch 1, Loss: 0.03536389023065567\n",
      "Epoch 1, Loss: 0.03550194948911667\n",
      "Epoch 1, Loss: 0.035991370677948\n",
      "Epoch 1, Loss: 0.03345311060547829\n",
      "Epoch 1, Loss: 0.03453997150063515\n",
      "Epoch 1, Loss: 0.03321407735347748\n",
      "Epoch 1, Loss: 0.03882182389497757\n",
      "Epoch 1, Loss: 0.03342358022928238\n",
      "Epoch 1, Loss: 0.07268501818180084\n",
      "Epoch 1, Loss: 0.034851592034101486\n",
      "Epoch 1, Loss: 0.03375964239239693\n",
      "Epoch 1, Loss: 0.03297802805900574\n",
      "Epoch 1, Loss: 0.028778618201613426\n",
      "Epoch 1, Loss: 0.029418373480439186\n",
      "Epoch 1, Loss: 0.03595447912812233\n",
      "Epoch 1, Loss: 0.034221455454826355\n",
      "Epoch 1, Loss: 0.03306284174323082\n",
      "Epoch 1, Loss: 0.03379429131746292\n",
      "Epoch 1, Loss: 0.034598153084516525\n",
      "Epoch 1, Loss: 0.030855955556035042\n",
      "Epoch 1, Loss: 0.029824715107679367\n",
      "Epoch 1, Loss: 0.07058466225862503\n",
      "Epoch 1, Loss: 0.032034024596214294\n",
      "Epoch 1, Loss: 0.03149423748254776\n",
      "Epoch 1, Loss: 0.03251563012599945\n",
      "Epoch 1, Loss: 0.028157318010926247\n",
      "Epoch 1, Loss: 0.03390922397375107\n",
      "Epoch 1, Loss: 0.030828068032860756\n",
      "Epoch 1, Loss: 0.028556350618600845\n",
      "Epoch 1, Loss: 0.03332322835922241\n",
      "Epoch 1, Loss: 0.04402096942067146\n",
      "Epoch 1, Loss: 0.030329344794154167\n",
      "Epoch 1, Loss: 0.0626697689294815\n",
      "Epoch 1, Loss: 0.03084626980125904\n",
      "Epoch 1, Loss: 0.03293450549244881\n",
      "Epoch 1, Loss: 0.031017201021313667\n",
      "Epoch 1, Loss: 0.032315246760845184\n",
      "Epoch 1, Loss: 0.031338535249233246\n",
      "Epoch 1, Loss: 0.03985388204455376\n",
      "Epoch 1, Loss: 0.028218520805239677\n",
      "Epoch 1, Loss: 0.03826991841197014\n",
      "Epoch 1, Loss: 0.035080086439847946\n",
      "Epoch 1, Loss: 0.028838584199547768\n",
      "Epoch 1, Loss: 0.03149482235312462\n",
      "Epoch 1, Loss: 0.05369877070188522\n",
      "Epoch 1, Loss: 0.031722597777843475\n",
      "Epoch 1, Loss: 0.03182462602853775\n",
      "Epoch 1, Loss: 0.02849683165550232\n",
      "Epoch 1, Loss: 0.029761962592601776\n",
      "Epoch 1, Loss: 0.030752001330256462\n",
      "Epoch 1, Loss: 0.035027503967285156\n",
      "Epoch 1, Loss: 0.031925518065690994\n",
      "Epoch 1, Loss: 0.027698703110218048\n",
      "Epoch 1, Loss: 0.029982594773173332\n",
      "Epoch 1, Loss: 0.03513140603899956\n",
      "Epoch 1, Loss: 0.0642768070101738\n",
      "Epoch 1, Loss: 0.029514066874980927\n",
      "Epoch 1, Loss: 0.0314367339015007\n",
      "Epoch 1, Loss: 0.030309949070215225\n",
      "Epoch 1, Loss: 0.03142473101615906\n",
      "Epoch 1, Loss: 0.028090806677937508\n",
      "Epoch 1, Loss: 0.034533604979515076\n",
      "Epoch 1, Loss: 0.028394917026162148\n",
      "Epoch 1, Loss: 0.02999665029346943\n",
      "Epoch 2, Loss: 0.027622543275356293\n",
      "Epoch 2, Loss: 0.031128380447626114\n",
      "Epoch 2, Loss: 0.027274219319224358\n",
      "Epoch 2, Loss: 0.028818944469094276\n",
      "Epoch 2, Loss: 0.02769564837217331\n",
      "Epoch 2, Loss: 0.027000773698091507\n",
      "Epoch 2, Loss: 0.028838707134127617\n",
      "Epoch 2, Loss: 0.029046285897493362\n",
      "Epoch 2, Loss: 0.026594121009111404\n",
      "Epoch 2, Loss: 0.02584129199385643\n",
      "Epoch 2, Loss: 0.02797451615333557\n",
      "Epoch 2, Loss: 0.027464598417282104\n",
      "Epoch 2, Loss: 0.02536734566092491\n",
      "Epoch 2, Loss: 0.029146792367100716\n",
      "Epoch 2, Loss: 0.026841862127184868\n",
      "Epoch 2, Loss: 0.02748124487698078\n",
      "Epoch 2, Loss: 0.02956436388194561\n",
      "Epoch 2, Loss: 0.026062408462166786\n",
      "Epoch 2, Loss: 0.027487898245453835\n",
      "Epoch 2, Loss: 0.026573877781629562\n",
      "Epoch 2, Loss: 0.028193948790431023\n",
      "Epoch 2, Loss: 0.0236084945499897\n",
      "Epoch 2, Loss: 0.027778344228863716\n",
      "Epoch 2, Loss: 0.024828799068927765\n",
      "Epoch 2, Loss: 0.02489880472421646\n",
      "Epoch 2, Loss: 0.025477170944213867\n",
      "Epoch 2, Loss: 0.028068775311112404\n",
      "Epoch 2, Loss: 0.02654959075152874\n",
      "Epoch 2, Loss: 0.02358875423669815\n",
      "Epoch 2, Loss: 0.026393897831439972\n",
      "Epoch 2, Loss: 0.02689758501946926\n",
      "Epoch 2, Loss: 0.02554485574364662\n",
      "Epoch 2, Loss: 0.02560354396700859\n",
      "Epoch 2, Loss: 0.02630656212568283\n",
      "Epoch 2, Loss: 0.026044098660349846\n",
      "Epoch 2, Loss: 0.0285932719707489\n",
      "Epoch 2, Loss: 0.025018509477376938\n",
      "Epoch 2, Loss: 0.026237333193421364\n",
      "Epoch 2, Loss: 0.023503195494413376\n",
      "Epoch 2, Loss: 0.026464859023690224\n",
      "Epoch 2, Loss: 0.025094322860240936\n",
      "Epoch 2, Loss: 0.02435857057571411\n",
      "Epoch 2, Loss: 0.02513071335852146\n",
      "Epoch 2, Loss: 0.024153253063559532\n",
      "Epoch 2, Loss: 0.023956257849931717\n",
      "Epoch 2, Loss: 0.022672777995467186\n",
      "Epoch 2, Loss: 0.024493802338838577\n",
      "Epoch 2, Loss: 0.02228689007461071\n",
      "Epoch 2, Loss: 0.025535868480801582\n",
      "Epoch 2, Loss: 0.02340053953230381\n",
      "Epoch 2, Loss: 0.0238985326141119\n",
      "Epoch 2, Loss: 0.02290881797671318\n",
      "Epoch 2, Loss: 0.0247675608843565\n",
      "Epoch 2, Loss: 0.02423478290438652\n",
      "Epoch 2, Loss: 0.024150414392352104\n",
      "Epoch 2, Loss: 0.025643667206168175\n",
      "Epoch 2, Loss: 0.025139397010207176\n",
      "Epoch 2, Loss: 0.022650469094514847\n",
      "Epoch 2, Loss: 0.02357988804578781\n",
      "Epoch 2, Loss: 0.026031089946627617\n",
      "Epoch 2, Loss: 0.02068565972149372\n",
      "Epoch 2, Loss: 0.02191437967121601\n",
      "Epoch 2, Loss: 0.02358684316277504\n",
      "Epoch 2, Loss: 0.021914606913924217\n",
      "Epoch 2, Loss: 0.02284085378050804\n",
      "Epoch 2, Loss: 0.0219008456915617\n",
      "Epoch 2, Loss: 0.023165641352534294\n",
      "Epoch 2, Loss: 0.021180560812354088\n",
      "Epoch 2, Loss: 0.02370431087911129\n",
      "Epoch 2, Loss: 0.022305194288492203\n",
      "Epoch 2, Loss: 0.0215986967086792\n",
      "Epoch 2, Loss: 0.024276355281472206\n",
      "Epoch 2, Loss: 0.022788014262914658\n",
      "Epoch 2, Loss: 0.022180013358592987\n",
      "Epoch 2, Loss: 0.02387988567352295\n",
      "Epoch 2, Loss: 0.023720452561974525\n",
      "Epoch 2, Loss: 0.022905830293893814\n",
      "Epoch 2, Loss: 0.02148323506116867\n",
      "Epoch 2, Loss: 0.02412756159901619\n",
      "Epoch 2, Loss: 0.02140742354094982\n",
      "Epoch 2, Loss: 0.02093324065208435\n",
      "Epoch 2, Loss: 0.021222790703177452\n",
      "Epoch 2, Loss: 0.02186463214457035\n",
      "Epoch 2, Loss: 0.0211881585419178\n",
      "Epoch 2, Loss: 0.021858185529708862\n",
      "Epoch 2, Loss: 0.022677937522530556\n",
      "Epoch 2, Loss: 0.02185891382396221\n",
      "Epoch 2, Loss: 0.021227575838565826\n",
      "Epoch 2, Loss: 0.021241320297122\n",
      "Epoch 2, Loss: 0.019685082137584686\n",
      "Epoch 2, Loss: 0.04206523671746254\n",
      "Epoch 2, Loss: 0.021326400339603424\n",
      "Epoch 2, Loss: 0.024079440161585808\n",
      "Epoch 2, Loss: 0.022571437060832977\n",
      "Epoch 2, Loss: 0.05765184015035629\n",
      "Epoch 2, Loss: 0.022308342158794403\n",
      "Epoch 2, Loss: 0.021048039197921753\n",
      "Epoch 2, Loss: 0.02417270466685295\n",
      "Epoch 2, Loss: 0.02342388406395912\n",
      "Epoch 2, Loss: 0.023454884067177773\n",
      "Epoch 2, Loss: 0.02417031116783619\n",
      "Epoch 2, Loss: 0.022548459470272064\n",
      "Epoch 2, Loss: 0.021600302308797836\n",
      "Epoch 2, Loss: 0.021132491528987885\n",
      "Epoch 2, Loss: 0.02068757452070713\n",
      "Epoch 2, Loss: 0.019671015441417694\n",
      "Epoch 2, Loss: 0.024915890768170357\n",
      "Epoch 2, Loss: 0.021367138251662254\n",
      "Epoch 2, Loss: 0.03254406526684761\n",
      "Epoch 2, Loss: 0.01980038918554783\n",
      "Epoch 2, Loss: 0.021960141137242317\n",
      "Epoch 2, Loss: 0.02026279829442501\n",
      "Epoch 2, Loss: 0.022669313475489616\n",
      "Epoch 2, Loss: 0.02416045218706131\n",
      "Epoch 2, Loss: 0.019899453967809677\n",
      "Epoch 2, Loss: 0.01992601342499256\n",
      "Epoch 2, Loss: 0.020807873457670212\n",
      "Epoch 2, Loss: 0.02269468456506729\n",
      "Epoch 2, Loss: 0.020100558176636696\n",
      "Epoch 2, Loss: 0.01998945139348507\n",
      "Epoch 2, Loss: 0.02191554754972458\n",
      "Epoch 2, Loss: 0.02184019610285759\n",
      "Epoch 2, Loss: 0.01983676478266716\n",
      "Epoch 2, Loss: 0.021291721612215042\n",
      "Epoch 2, Loss: 0.018736088648438454\n",
      "Epoch 2, Loss: 0.04385072737932205\n",
      "Epoch 2, Loss: 0.018995413556694984\n",
      "Epoch 2, Loss: 0.020663287490606308\n",
      "Epoch 2, Loss: 0.021226191893219948\n",
      "Epoch 2, Loss: 0.02150239422917366\n",
      "Epoch 2, Loss: 0.020214956253767014\n",
      "Epoch 2, Loss: 0.020120739936828613\n",
      "Epoch 2, Loss: 0.018366241827607155\n",
      "Epoch 2, Loss: 0.021049145609140396\n",
      "Epoch 2, Loss: 0.0188167504966259\n",
      "Epoch 2, Loss: 0.020099978893995285\n",
      "Epoch 2, Loss: 0.022636311128735542\n",
      "Epoch 2, Loss: 0.06347881257534027\n",
      "Epoch 2, Loss: 0.020524414256215096\n",
      "Epoch 2, Loss: 0.019691986963152885\n",
      "Epoch 2, Loss: 0.02249857783317566\n",
      "Epoch 2, Loss: 0.018674571067094803\n",
      "Epoch 2, Loss: 0.019335361197590828\n",
      "Epoch 2, Loss: 0.019998295232653618\n",
      "Epoch 2, Loss: 0.0189337320625782\n",
      "Epoch 2, Loss: 0.01932263746857643\n",
      "Epoch 2, Loss: 0.016766561195254326\n",
      "Epoch 2, Loss: 0.019768668338656425\n",
      "Epoch 2, Loss: 0.019663339480757713\n",
      "Epoch 2, Loss: 0.020551707595586777\n",
      "Epoch 2, Loss: 0.020256835967302322\n",
      "Epoch 2, Loss: 0.025307495146989822\n",
      "Epoch 2, Loss: 0.018947478383779526\n",
      "Epoch 2, Loss: 0.017890552058815956\n",
      "Epoch 2, Loss: 0.01753423735499382\n",
      "Epoch 2, Loss: 0.020143430680036545\n",
      "Epoch 2, Loss: 0.020013127475976944\n",
      "Epoch 2, Loss: 0.01850254088640213\n",
      "Epoch 2, Loss: 0.018327483907341957\n",
      "Epoch 2, Loss: 0.01878179796040058\n",
      "Epoch 2, Loss: 0.022279147058725357\n",
      "Epoch 2, Loss: 0.018872443586587906\n",
      "Epoch 2, Loss: 0.018612168729305267\n",
      "Epoch 2, Loss: 0.019059931859374046\n",
      "Epoch 2, Loss: 0.018223857507109642\n",
      "Epoch 2, Loss: 0.016452794894576073\n",
      "Epoch 2, Loss: 0.0195553470402956\n",
      "Epoch 2, Loss: 0.018055051565170288\n",
      "Epoch 2, Loss: 0.02112669125199318\n",
      "Epoch 2, Loss: 0.01701372116804123\n",
      "Epoch 2, Loss: 0.016824103891849518\n",
      "Epoch 2, Loss: 0.017934264615178108\n",
      "Epoch 2, Loss: 0.017673637717962265\n",
      "Epoch 2, Loss: 0.017745446413755417\n",
      "Epoch 2, Loss: 0.019117312505841255\n",
      "Epoch 2, Loss: 0.018369315192103386\n",
      "Epoch 2, Loss: 0.018507948145270348\n",
      "Epoch 2, Loss: 0.017183249816298485\n",
      "Epoch 2, Loss: 0.017472796142101288\n",
      "Epoch 2, Loss: 0.01516794878989458\n",
      "Epoch 2, Loss: 0.01725330948829651\n",
      "Epoch 2, Loss: 0.01720108464360237\n",
      "Epoch 2, Loss: 0.016610771417617798\n",
      "Epoch 2, Loss: 0.018491363152861595\n",
      "Epoch 2, Loss: 0.017550082877278328\n",
      "Epoch 2, Loss: 0.017313670367002487\n",
      "Epoch 2, Loss: 0.04100876674056053\n",
      "Epoch 2, Loss: 0.019695082679390907\n",
      "Epoch 2, Loss: 0.018338462337851524\n",
      "Epoch 2, Loss: 0.016627591103315353\n",
      "Epoch 2, Loss: 0.016646230593323708\n",
      "Model weights saved to bert_multi_label_model.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# 读取上传的CSV文件\n",
    "file_path = '1.csv'\n",
    "data = pd.read_csv(file_path, encoding='gbk')\n",
    "\n",
    "# 查看数据\n",
    "print(data.head())\n",
    "\n",
    "# 假设我们有两个标签：籍贯(0)和出生年份(1)\n",
    "num_labels = len(data['label'].unique())\n",
    "\n",
    "# 创建多标签向量\n",
    "def create_multi_label(row):\n",
    "    multi_label = [0] * num_labels\n",
    "    multi_label[row['label']] = 1\n",
    "    return multi_label\n",
    "\n",
    "data['multi_label'] = data.apply(create_multi_label, axis=1)\n",
    "\n",
    "# 准备数据集\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = labels\n",
    "\n",
    "        return item\n",
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 创建数据集\n",
    "texts = data['question'].tolist()  # 确保使用正确的列名\n",
    "labels = data['multi_label'].tolist()  # 确保使用多标签列\n",
    "max_len = 128\n",
    "\n",
    "dataset = MultiLabelDataset(texts, labels, tokenizer, max_len)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = 'bert_multi_label_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model weights saved to {model_save_path}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T06:39:06.016237Z",
     "start_time": "2024-06-06T06:08:34.461289Z"
    }
   },
   "id": "15ad2d637ac22ff5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 加载模型并进行推理\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    text = \"a的别名是什么？\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    predicted_labels = (probabilities > 0.5).int()\n",
    "    print(predicted_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T06:45:20.170339Z",
     "start_time": "2024-06-06T06:45:20.070343Z"
    }
   },
   "id": "1c74e2733b4cead1",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# 加载预训练的SpaCy模型\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "# 示例问题列表\n",
    "questions = [\n",
    "    \"李鸿章的出生日期和官职是什么？\",\n",
    "    \"请问李鸿章的出生日期和官职是什么？\",\n",
    "    \"李鸿章的生日和职位是什么？\",\n",
    "    \"你知道李鸿章的出生日期和官职吗？\",\n",
    "    \"李鸿章的生日和他的职位是什么？\",\n",
    "    # 继续添加其他问题\n",
    "]\n",
    "\n",
    "# 定义一个函数来识别人名\n",
    "def extract_name(question):\n",
    "    doc = nlp(question)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            return ent.text\n",
    "    return None\n",
    "\n",
    "# 创建一个包含识别结果的DataFrame\n",
    "data = {'问题编号': range(1, len(questions) + 1), '问题': questions, '识别出的人名': [extract_name(q) for q in questions]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 保存为CSV文件\n",
    "df.to_csv('questions_with_names.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff7106fa11f6d008"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
